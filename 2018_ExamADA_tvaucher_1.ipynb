{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers.helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Significant Bang Theory\n",
    "\n",
    "Attention, ADA students!\n",
    "\n",
    "The Sheldon Cooper we all know and love (OK, some of us might not know him, and some might not love him) from the TV series \"The Big Bang Theory\" has gotten into an argument with Leonard from the same TV show. Sheldon insists that he knows the show better than anyone, and keeps making various claims about the show, which neither of them know how to prove or disprove. The two of them have reached out to you ladies and gentlemen, as data scientists, to help them. You will be given the full script of the series, with information on the episode, the scene, the person saying each dialogue line, and the dialogue lines themselves.\n",
    "\n",
    "Leonard has challenged several of Sheldon's claims about the show, and throughout this exam you will see some of those and you will get to prove or disprove them, but remember: sometimes, we can neither prove a claim, nor disprove it!\n",
    "\n",
    "## Deadline\n",
    "Wednesday, January 30th, 2019; 11:15 A.M. (Swiss time)\n",
    "\n",
    "_For the deadline for extramural exams, see the submission subsection._\n",
    "\n",
    "## Important notes\n",
    "* Don't forget to add a textual description of your thought process, the assumptions you made, and your results!\n",
    "* Please write all your comments in English, and use meaningful variable names in your code.\n",
    "* As we have seen during the semester, data science is all about multiple iterations on the same dataset. Do not obsess over small details in the beginning, and try to complete as many tasks as possible during the first 2 hours. Then, go back to the obtained results, write meaningful comments, and debug your code if you have found any glaring mistake.\n",
    "* Fully read the instructions for each question before starting to solve it to avoid misunderstandings, and remember to save your notebook often!\n",
    "* The exam contains **15 questions organised into 4 tasks**, and is designed for more than 3 hours. **You do not need to solve everything in order to get a 6**, and you have some freedom is choosing the tasks you wish to solve.\n",
    "* You cannot leave the room in the first and last 15 minutes.\n",
    "* You can use all the online resources you want except for communication tools (emails, web chats, forums, phone, etc.). We will be monitoring the network for unusual activity.\n",
    "* Remember, this is not a homework assignment -- no teamwork allowed!\n",
    "\n",
    "## Submission\n",
    "* Your file has to be named as \"NameSurname_SCIPER.ipynb\".\n",
    "* Make sure you upload your Jupyter Notebook (1 file) to [this](https://goo.gl/forms/7GLvYl94uSOn54jH2) Google form at the end of the exam, with all the cells already evaluated (except for the Spark-related question, Q7). You need to sign in to Google using your EPFL credentials in order to submit the form.\n",
    "* In case of problems with the form, send your Jupyter Notebook (along with your name and SCIPER number) as a direct message to @ramtin on Mattermost. This is reserved only for those who encounter problems with the submission -- you need to have a reasonable justification for using this back-up.\n",
    "* You will have until 11:20 (strict deadline) to turn in your submission. **Late submissions will not be accepted.** This deadline is for the students taking the exam at EPFL -- students taking the exam extramurally will have their submission deadline as the starting time of the exam plus 3 hours and 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A: Picking up the shovel (10 points)\n",
    "\n",
    "**Note: You will use the data you preprocess in this task in all the subsequent ones.**\n",
    "\n",
    "Our friends' argument concerns the entire show. We have given you a file in the `data/` folder that contains the script of every single episode. New episodes are indicated by '>>', new scenes by '>', and the rest of the lines are dialogue lines. Some lines are said by multiple people (for example, lines indicated by 'All' or 'Together'); **you must discard these lines**, for the sake of simplicity. However, you do not need to do it for Q1 in this task -- you'll take care of it when you solve Q2.\n",
    "\n",
    "**Q1**. (5 points) Your first task is to extract all lines of dialogue in each scene and episode, creating a dataframe where each row has the episode and scene where a dialogue line was said, the character who said it, and the line itself. You do not need to extract the proper name of the episode (e.g. episode 1 can appear as \"Series 01 Episode 01 - Pilot Episode\", and doesn't need to appear as \"Pilot Episode\"). Then, answer the following question: In total, how many scenes are there in each season? We're not asking about unique scenes; the same location appearing in two episodes counts as two scenes. You can use a Pandas dataframe with a season column and a scene count column as the response.\n",
    "\n",
    "**Note: The data refers to seasons as \"series\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "scene_id = 0\n",
    "with open('data/all_scripts.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>>'):\n",
    "            episode = line[2:].strip()\n",
    "        elif line.startswith('>'):\n",
    "            scene = line[1:].strip()\n",
    "            scene_id += 1\n",
    "        else:\n",
    "            char, text = line.split(':', 1)\n",
    "            df_list.append({'episode': episode, 'scene': scene, 'scene_id': scene_id, 'character': char, 'line': text.strip()})\n",
    "df = pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "      <th>scene_id</th>\n",
       "      <th>character</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>So if a photon is directed through a plane wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>Agreed, what’s your point?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>There’s no point, I just think it’s a good ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>Excuse me?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Hang on.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51287</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>2805</td>\n",
       "      <td>Ramona</td>\n",
       "      <td>Mmm. No big deal, I enjoy spending time with you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51288</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>2805</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>And I with you. Question, are you seeking a ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51289</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>2805</td>\n",
       "      <td>Ramona</td>\n",
       "      <td>What if I were?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51290</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>2805</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Well, that would raise a number of problems. W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51291</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Princeton.</td>\n",
       "      <td>2806</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>(Knock, knock, knock) Amy. (Knock, knock, knoc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51292 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 episode  \\\n",
       "0                   Series 01 Episode 01 – Pilot Episode   \n",
       "1                   Series 01 Episode 01 – Pilot Episode   \n",
       "2                   Series 01 Episode 01 – Pilot Episode   \n",
       "3                   Series 01 Episode 01 – Pilot Episode   \n",
       "4                   Series 01 Episode 01 – Pilot Episode   \n",
       "...                                                  ...   \n",
       "51287  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51288  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51289  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51290  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51291  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "\n",
       "                             scene  scene_id     character  \\\n",
       "0      A corridor at a sperm bank.         1       Sheldon   \n",
       "1      A corridor at a sperm bank.         1       Leonard   \n",
       "2      A corridor at a sperm bank.         1       Sheldon   \n",
       "3      A corridor at a sperm bank.         1       Leonard   \n",
       "4      A corridor at a sperm bank.         1  Receptionist   \n",
       "...                            ...       ...           ...   \n",
       "51287            Sheldon’s office.      2805        Ramona   \n",
       "51288            Sheldon’s office.      2805       Sheldon   \n",
       "51289            Sheldon’s office.      2805        Ramona   \n",
       "51290            Sheldon’s office.      2805       Sheldon   \n",
       "51291                   Princeton.      2806       Sheldon   \n",
       "\n",
       "                                                    line  \n",
       "0      So if a photon is directed through a plane wit...  \n",
       "1                             Agreed, what’s your point?  \n",
       "2      There’s no point, I just think it’s a good ide...  \n",
       "3                                             Excuse me?  \n",
       "4                                               Hang on.  \n",
       "...                                                  ...  \n",
       "51287  Mmm. No big deal, I enjoy spending time with you.  \n",
       "51288  And I with you. Question, are you seeking a ro...  \n",
       "51289                                    What if I were?  \n",
       "51290  Well, that would raise a number of problems. W...  \n",
       "51291  (Knock, knock, knock) Amy. (Knock, knock, knoc...  \n",
       "\n",
       "[51292 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'] = df.episode.apply(lambda x: int(x[7:9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I assume that the same location in the same episode but separated by a '>' count as 2 different scenes (not clear in the instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_scene</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        nb_scene\n",
       "season          \n",
       "1            160\n",
       "2            231\n",
       "3            236\n",
       "4            279\n",
       "5            254\n",
       "6            304\n",
       "7            332\n",
       "8            327\n",
       "9            337\n",
       "10           346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('season').agg(nb_scene = ('scene_id', pd.Series.nunique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. (5 points) Now, let's define two sets of characters: all the characters, and recurrent characters. Recurrent characters are those who appear in more than one episode. For the subsequent sections, you will need to have a list of recurrent characters. Assume that there are no two _named characters_ (i.e. characters who have actual names and aren't referred to generically as \"little girl\", \"grumpy grandpa\", etc.) with the same name, i.e. there are no two Sheldons, etc. Generate a list of recurrent characters who have more than 90 dialogue lines in total, and then take a look at the list you have. If you've done this correctly, you should have a list of 20 names. However, one of these is clearly not a recurrent character. Manually remove that one, and print out your list of recurrent characters. To remove that character, pay attention to the _named character_ assumption we gave you earlier on. **For all the subsequent questions, you must only keep the dialogue lines said by the recurrent characters in your list.**\n",
    "\n",
    "_Hint: \"I know all the recurrent characters because I've watched the entire series five times\" is not an acceptable argument, so you need to actually generate the list._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_nb_line = df.groupby('character').agg(nb_line = ('line', pd.Series.count))\n",
    "char_nb_line = char_nb_line[char_nb_line.nb_line >= 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_line</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>character</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amy</th>\n",
       "      <td>3472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arthur</th>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernadette</th>\n",
       "      <td>2690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bert</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beverley</th>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Howard</th>\n",
       "      <td>5872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kripke</th>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leonard</th>\n",
       "      <td>9829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leslie</th>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Man</th>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Cooper</th>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mrs Wolowitz</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Penny</th>\n",
       "      <td>7677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Priya</th>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Raj</th>\n",
       "      <td>4779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sheldon</th>\n",
       "      <td>11689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stuart</th>\n",
       "      <td>733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wil</th>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zack</th>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nb_line\n",
       "character            \n",
       "Amy              3472\n",
       "Arthur            130\n",
       "Bernadette       2690\n",
       "Bert               95\n",
       "Beverley          162\n",
       "Emily             164\n",
       "Howard           5872\n",
       "Kripke            106\n",
       "Leonard          9829\n",
       "Leslie            116\n",
       "Man               105\n",
       "Mrs Cooper        213\n",
       "Mrs Wolowitz      136\n",
       "Penny            7677\n",
       "Priya             222\n",
       "Raj              4779\n",
       "Sheldon         11689\n",
       "Stuart            733\n",
       "Wil               126\n",
       "Zack              135"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_nb_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man clearly isn't a named character, so we will remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "      <th>scene_id</th>\n",
       "      <th>character</th>\n",
       "      <th>line</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>So if a photon is directed through a plane wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>Agreed, what’s your point?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>There’s no point, I just think it’s a good ide...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>Excuse me?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>One across is Aegean, eight down is Nabakov, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51284</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>2805</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Uh, breakfast yes, lunch no. I did have a coug...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51286</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>2805</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>How thoughtful. Thank you.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51288</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>2805</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>And I with you. Question, are you seeking a ro...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51290</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>2805</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Well, that would raise a number of problems. W...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51291</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Princeton.</td>\n",
       "      <td>2806</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>(Knock, knock, knock) Amy. (Knock, knock, knoc...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48346 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 episode  \\\n",
       "0                   Series 01 Episode 01 – Pilot Episode   \n",
       "1                   Series 01 Episode 01 – Pilot Episode   \n",
       "2                   Series 01 Episode 01 – Pilot Episode   \n",
       "3                   Series 01 Episode 01 – Pilot Episode   \n",
       "5                   Series 01 Episode 01 – Pilot Episode   \n",
       "...                                                  ...   \n",
       "51284  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51286  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51288  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51290  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51291  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "\n",
       "                             scene  scene_id character  \\\n",
       "0      A corridor at a sperm bank.         1   Sheldon   \n",
       "1      A corridor at a sperm bank.         1   Leonard   \n",
       "2      A corridor at a sperm bank.         1   Sheldon   \n",
       "3      A corridor at a sperm bank.         1   Leonard   \n",
       "5      A corridor at a sperm bank.         1   Leonard   \n",
       "...                            ...       ...       ...   \n",
       "51284            Sheldon’s office.      2805   Sheldon   \n",
       "51286            Sheldon’s office.      2805   Sheldon   \n",
       "51288            Sheldon’s office.      2805   Sheldon   \n",
       "51290            Sheldon’s office.      2805   Sheldon   \n",
       "51291                   Princeton.      2806   Sheldon   \n",
       "\n",
       "                                                    line  season  \n",
       "0      So if a photon is directed through a plane wit...       1  \n",
       "1                             Agreed, what’s your point?       1  \n",
       "2      There’s no point, I just think it’s a good ide...       1  \n",
       "3                                             Excuse me?       1  \n",
       "5      One across is Aegean, eight down is Nabakov, t...       1  \n",
       "...                                                  ...     ...  \n",
       "51284  Uh, breakfast yes, lunch no. I did have a coug...      10  \n",
       "51286                         How thoughtful. Thank you.      10  \n",
       "51288  And I with you. Question, are you seeking a ro...      10  \n",
       "51290  Well, that would raise a number of problems. W...      10  \n",
       "51291  (Knock, knock, knock) Amy. (Knock, knock, knoc...      10  \n",
       "\n",
       "[48346 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_char = char_nb_line.index.to_list()\n",
    "rec_char.remove('Man')\n",
    "df_rec = df[df.character.isin(rec_char)].copy()\n",
    "df_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B: Read the ~~stats~~ scripts carefully (30 points)\n",
    "\n",
    "### Part 1: Don't put the shovel down just yet\n",
    "\n",
    "**Q3**. (2.5 points) From each dialogue line, replace punctuation marks (listed in the EXCLUDE_CHARS variable provided in `helpers/helper_functions.py`) with whitespaces, and lowercase all the text. **Do not remove any stopwords, leave them be for all the questions in this task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\\\?:>!\\\\{\\\\#%\\\\)\\\\]<\\\\-\\\\}\\\\*/\\'\\\\^\\\\\\\\\\\\$\\\\&\\\\|,\\\\(@;`=\\\\[_\\\\~’\\\\+\\\\.\"]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'[{}]'.format(re.escape(''.join(EXCLUDE_CHARS)))\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec['line_processed'] = df_rec.line.str.lower().str.replace(pattern, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "      <th>scene_id</th>\n",
       "      <th>character</th>\n",
       "      <th>line</th>\n",
       "      <th>season</th>\n",
       "      <th>line_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>So if a photon is directed through a plane wit...</td>\n",
       "      <td>1</td>\n",
       "      <td>so if a photon is directed through a plane wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>Agreed, what’s your point?</td>\n",
       "      <td>1</td>\n",
       "      <td>agreed  what s your point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>There’s no point, I just think it’s a good ide...</td>\n",
       "      <td>1</td>\n",
       "      <td>there s no point  i just think it s a good ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>Excuse me?</td>\n",
       "      <td>1</td>\n",
       "      <td>excuse me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>1</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>One across is Aegean, eight down is Nabakov, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>one across is aegean  eight down is nabakov  t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                episode                        scene  \\\n",
       "0  Series 01 Episode 01 – Pilot Episode  A corridor at a sperm bank.   \n",
       "1  Series 01 Episode 01 – Pilot Episode  A corridor at a sperm bank.   \n",
       "2  Series 01 Episode 01 – Pilot Episode  A corridor at a sperm bank.   \n",
       "3  Series 01 Episode 01 – Pilot Episode  A corridor at a sperm bank.   \n",
       "5  Series 01 Episode 01 – Pilot Episode  A corridor at a sperm bank.   \n",
       "\n",
       "   scene_id character                                               line  \\\n",
       "0         1   Sheldon  So if a photon is directed through a plane wit...   \n",
       "1         1   Leonard                         Agreed, what’s your point?   \n",
       "2         1   Sheldon  There’s no point, I just think it’s a good ide...   \n",
       "3         1   Leonard                                         Excuse me?   \n",
       "5         1   Leonard  One across is Aegean, eight down is Nabakov, t...   \n",
       "\n",
       "   season                                     line_processed  \n",
       "0       1  so if a photon is directed through a plane wit...  \n",
       "1       1                         agreed  what s your point   \n",
       "2       1  there s no point  i just think it s a good ide...  \n",
       "3       1                                         excuse me   \n",
       "5       1  one across is aegean  eight down is nabakov  t...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**. (5 points) For each term, calculate its \"corpus frequency\", i.e. its number of occurrences in the entire series. Visualize the distribution of corpus frequency using a histogram. Explain your observations. What are the appropriate x and y scales for this plot?\n",
    "\n",
    "Here we assume we split on whitespaces `'\\W+'` and we remove final whitespace (eg. there was punc at the end of the sentence) using `strip` so we don't get the empty token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "df_rec.line_processed.str.strip().str.split('\\W+').explode().value_counts().hist(log=True, bins=20, ax=ax)\n",
    "ax.set_title('Corpus frequency distribution')\n",
    "ax.set_ylabel('Number of terms with given frequency')\n",
    "ax.set_xlabel('Corpus frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get a heavy tailed distribution, with a lot of terms appearing rarely (more than 10^4 in the first bin) and a few terms (less than 20) that appears more than 10'000 times each. Those should be considered as stopwords in a later phase (like 'i', 'you', 'the', ...). We use a Log scale for the y axis to show the difference between the number of terms with a given frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Talkativity\n",
    "**Q5**. (2.5 points) For each of the recurrent characters, calculate their total number of words uttered across all episodes. Based on this, who seems to be the most talkative character?\n",
    "\n",
    "Similarly we split on `\\W+` and remove trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character\n",
       "Sheldon         185543\n",
       "Leonard         102649\n",
       "Penny            79374\n",
       "Howard           69577\n",
       "Raj              60154\n",
       "Amy              39963\n",
       "Bernadette       27746\n",
       "Stuart            7966\n",
       "Mrs Cooper        3396\n",
       "Beverley          2029\n",
       "Priya             1942\n",
       "Wil               1681\n",
       "Emily             1573\n",
       "Mrs Wolowitz      1460\n",
       "Arthur            1452\n",
       "Zack              1428\n",
       "Leslie            1252\n",
       "Kripke            1246\n",
       "Bert              1146\n",
       "Name: line_processed, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec.groupby('character')['line_processed']\\\n",
    "      .apply(lambda x: x.str.strip().str.split('\\W+').explode().count())\\\n",
    "      .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, Sheldon seems to be the most talkative, but this makes sense as he also has the most lines (11689, cf. Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**. (12.5 points) For each of the recurrent characters, calculate their total number of words uttered per episode (ignoring episodes that the character does not appear in), and calculate a **robust summary statistic** for the word count distribution of each person.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**i)** (2.5 points) What changes do you observe, compared to the analysis in Q5?\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**ii)** (2.5 points) Why is this analysis an improvement over the previous one, and how could you improve it even further? _Hint: The improvement involves making your unit for word counts even more granular - you can go further down than episodes._\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**iii)** (7.5 points) Incorporate that improvement. Do you still see the same results? How **confident** can you be that the \"most talkative\" person given by this twice improved method is really more talkative than the second most talkative one? _Hint: Read the question again. A good idea would be to use bootstrapping and calculate your summary statistic on each bootstrapped set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character  episode                                            \n",
       "Amy        Series 03 Episode 23 – The Lunar Excitation            103\n",
       "           Series 04 Episode 01 – The Robotic Manipulation        144\n",
       "           Series 04 Episode 03 – The Zazzy Substitution          473\n",
       "           Series 04 Episode 05 – The Desperation Emanation       229\n",
       "           Series 04 Episode 08 – The 21 Second Excitation        602\n",
       "                                                                 ... \n",
       "Zack       Series 04 Episode 17 – The Toast Derivation            154\n",
       "           Series 07 Episode 09 – The Thanksgiving Decoupling     126\n",
       "           Series 07 Episode 11 – The Cooper Extraction            17\n",
       "           Series 09 Episode 22 – The Fermentation Bifurcation    218\n",
       "           Series 10 Episode 22 – The Cognition Regeneration      282\n",
       "Name: line_processed, Length: 1641, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_per_episode_per_char = \\\n",
    "    df_rec.groupby(['character', 'episode'])['line_processed']\\\n",
    "          .apply(lambda x: x.str.strip().str.split('\\W+').explode().count())\n",
    "word_per_episode_per_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy **robust summary statistic** that we could use is the **median** of the number of word per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character\n",
       "Sheldon         777.0\n",
       "Leonard         398.0\n",
       "Arthur          325.0\n",
       "Penny           323.0\n",
       "Mrs Cooper      295.5\n",
       "Howard          289.0\n",
       "Amy             234.0\n",
       "Raj             219.5\n",
       "Bert            199.0\n",
       "Beverley        184.0\n",
       "Kripke          162.0\n",
       "Bernadette      157.0\n",
       "Priya           145.0\n",
       "Zack            140.0\n",
       "Wil             122.5\n",
       "Leslie          121.0\n",
       "Stuart          112.5\n",
       "Emily            87.0\n",
       "Mrs Wolowitz     39.0\n",
       "Name: line_processed, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_per_episode_per_char.groupby('character').median().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i)** Sheldon and Leonard, stay as the top 2, but the rest of the top 5 changes with Arthur and Mrs Cooper climbing up. Penny and Howard both lose some places.\n",
    "\n",
    "**ii)** The top 2 makes sense because they have the most lines and appear in most episode. Arthur and Mrs Cooper don't have many lines (less than 220) but seems to speak a lot and don't appear often, so it makes sense that they climb up the leaderboard. The other that appear often but don't talk much (like Penny and Howard) seems to lose some places. This is already better because we take now into account that everyone doesn't appear as much as one another. IMO, an even better option would be to compute the median number of words per line and not per episode to get who's the most talkative as fairly as possible\n",
    "\n",
    "**iii)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character\n",
       "Sheldon         12\n",
       "Bert            12\n",
       "Beverley        11\n",
       "Mrs Cooper      11\n",
       "Kripke          10\n",
       "Raj             10\n",
       "Zack             9\n",
       "Arthur           9\n",
       "Howard           9\n",
       "Amy              9\n",
       "Wil              9\n",
       "Mrs Wolowitz     9\n",
       "Stuart           9\n",
       "Leonard          8\n",
       "Penny            8\n",
       "Bernadette       8\n",
       "Leslie           8\n",
       "Emily            7\n",
       "Priya            7\n",
       "Name: words_line, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec['words_line'] = df_rec.line_processed.str.strip().str.split('\\W+').apply(len)\n",
    "df_rec.groupby('character').words_line.median().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: perform bootstrap resampling on df, compute median for each character, at the end perform 95% CI interval and compare (can even use pairwise t-test if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Obligatory Spark cameo\n",
    "**Q7**. (7.5 points) Write a Spark script that does the following: Given the raw input file and your list of recurrent characters, create an RDD containing (speaker, dialogue line) rows **only for the recurrent characters** (assume that you already have the list --  no need to calculate it using Spark), and then generate a vectorized bag of words representation for each dialogue line, thus generating an RDD with (speaker, bag of words vector) rows. Then, calculate an aggregated bag of words vector (sum of all vectors) for each person. The final output is therefore an RDD with each of its rows being (speaker, aggregated bag of words vector). For your bag of words vectors, you can use $1\\times|V|$ scipy CSR matrices (where $|V|$ is the size of the vocabulary). No filtering of the vocabulary is necessary for this part.\n",
    "\n",
    "You do not need to run this script, but you do need to use Spark logic and also, the syntax needs to be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create the session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# create the context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile('data/all_scripts.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_clean(x):\n",
    "    a, b = x.split(':', 1)\n",
    "    return a, b.strip()\n",
    "\n",
    "# 1. Create an RDD containing (speaker, dialogue line) rows only for the recurrent characters\n",
    "char_line_rdd = file.filter(lambda x: not x.startswith('>')).map(split_clean).filter(lambda x: x[0] in rec_char).persist()\n",
    "# 2. Build vocabulary for BoW\n",
    "vocabulary = dict(char_line_rdd.flatMap(lambda x: x[1].split(' ')).distinct().zipWithIndex().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sheldon', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 24671 stored elements in Compressed Sparse Row format>),\n",
       " ('Leonard', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 12701 stored elements in Compressed Sparse Row format>),\n",
       " ('Howard', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 10827 stored elements in Compressed Sparse Row format>),\n",
       " ('Leslie', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 657 stored elements in Compressed Sparse Row format>),\n",
       " ('Kripke', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 672 stored elements in Compressed Sparse Row format>),\n",
       " ('Beverley', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 885 stored elements in Compressed Sparse Row format>),\n",
       " ('Wil', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 672 stored elements in Compressed Sparse Row format>),\n",
       " ('Mrs Wolowitz', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 646 stored elements in Compressed Sparse Row format>),\n",
       " ('Zack', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 657 stored elements in Compressed Sparse Row format>),\n",
       " ('Amy', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 7302 stored elements in Compressed Sparse Row format>),\n",
       " ('Priya', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 831 stored elements in Compressed Sparse Row format>),\n",
       " ('Arthur', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 617 stored elements in Compressed Sparse Row format>),\n",
       " ('Penny', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 9418 stored elements in Compressed Sparse Row format>),\n",
       " ('Raj', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 9779 stored elements in Compressed Sparse Row format>),\n",
       " ('Mrs Cooper', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 1261 stored elements in Compressed Sparse Row format>),\n",
       " ('Stuart', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 2207 stored elements in Compressed Sparse Row format>),\n",
       " ('Bernadette', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 4947 stored elements in Compressed Sparse Row format>),\n",
       " ('Bert', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 533 stored elements in Compressed Sparse Row format>),\n",
       " ('Emily', <1x42565 sparse matrix of type '<class 'numpy.int32'>'\n",
       "  \twith 645 stored elements in Compressed Sparse Row format>)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def create_bow(x, vocabulary):\n",
    "    ''' helper method to create the BoW csr matrix'''\n",
    "    counter = Counter(x.split(' '))\n",
    "    data = []\n",
    "    col_ind = []\n",
    "    for k, v in counter.items():\n",
    "        data.append(v)\n",
    "        col_ind.append(vocabulary[k])\n",
    "    return csr_matrix((data, (np.zeros_like(data), col_ind)), shape=(1, len(vocabulary)))\n",
    "\n",
    "voc_bc = sc.broadcast(vocabulary)\n",
    "# 3. Generate an RDD with (speaker, bag of words vector) rows.\n",
    "char_line_bow = char_line_rdd.map(lambda x: (x[0], create_bow(x[1], voc_bc.value))).persist()\n",
    "\n",
    "# 4. Calculate an aggregated bag of words vector (sum of all vectors) for each person\n",
    "agg_char_bow = char_line_bow.reduceByKey(lambda x, y: x + y)\n",
    "agg_char_bow.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C: The Gossip Graph (30 points)\n",
    "\n",
    "**Note: Only for this task, discard the recurrent characters whose names are not single words, e.g. Mrs. Cooper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec_c = df_rec[~df_rec.character.str.startswith('Mrs')].copy()\n",
    "rec_char_c = df_rec_c.character.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define _gossip_ as follows: if a dialogue line of character A mentions B by name in a scene that does not involve character B, we say that “A gossips about B” in that line. Multiple mentions of the same person in a single line are counted once, but a character can gossip about several others in the same line. For the sake of simplicity, we only consider gossips where the name of the recurrent character is mentioned as it appears in our list of characters; for example, if someone says \"Cooper\" and they mean Sheldon, we discard that.\n",
    "\n",
    "**Q8**. (12.5 points) Create the two following graphs first:\n",
    "\n",
    "1. (5 points) Create the _familiarity graph_, an undirected weighted graph, in which there is a node for each recurrent character, and an edge between two characters if they appear together in at least one scene. The weight of the edge between them is the number of scenes they appear in together. If an edge exists between two people in the familiarity graph, we say that they \"know each other\".\n",
    "2. (7.5 points) Create the _gossip graph_, which is a directed weighted graph, in which there there is a node for each recurrent character, and a directed edge from the node for A to the node for B if A has gossiped about B at least once. The weight of the edge is the number of scenes in which A has gossiped about B.\n",
    "\n",
    "_Hint: You can create each graph first as an adjacency matrix and then create a networkx graph out of that._\n",
    "\n",
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_char = len(rec_char_c)\n",
    "dict_char = dict(zip(rec_char_c, range(nb_char)))\n",
    "adj_matrix_familiarity = np.zeros((nb_char, nb_char), dtype=int)\n",
    "for char_set in df_rec_c.groupby('scene_id').character.apply(set):\n",
    "    if len(char_set) == 1: pass;\n",
    "    for char_1 in char_set:\n",
    "        for char_2 in char_set:\n",
    "            if char_1 != char_2:\n",
    "                adj_matrix_familiarity[dict_char[char_1], dict_char[char_2]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "familirity_graph = nx.Graph(adj_matrix_familiarity)\n",
    "familirity_graph = nx.relabel_nodes(familirity_graph, dict(enumerate(rec_char_c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gossip_line(x):\n",
    "    gossip = set()\n",
    "    for char in rec_char_c:\n",
    "        if char in x:\n",
    "            gossip.add(char)\n",
    "    return gossip\n",
    "\n",
    "df_rec_c['gossip'] = df_rec_c.line.apply(gossip_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gossip_df = df_rec_c.groupby(['scene_id', 'character']).gossip.apply(lambda x: set().union(*x)).reset_index(level='character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix_gossip = np.zeros((nb_char, nb_char), dtype=int)\n",
    "for _, row in gossip_df.iterrows():\n",
    "    for char in row['gossip']:\n",
    "        if char != row['character']: #cannot gossip about oneself\n",
    "            adj_matrix_gossip[dict_char[row['character']], dict_char[char]] += 1\n",
    "\n",
    "gossip_graph = nx.DiGraph(adj_matrix_gossip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, answer the following questions:\n",
    "\n",
    "**Q9**. (5 points) Sheldon claims that every character in the show is familiar with everyone else through at most one intermediary. Based on the familiarity graph, is this true? If not, at most how many intermediaries are needed?\n",
    "\n",
    "To answer that means to find the longest shortest past between all characters, if this is <= 2, then Sheldon is True as there's at most 1 intermediary node between the 2 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum len shortest path is 2\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for _, dict_path in nx.shortest_path_length(familirity_graph):\n",
    "    for length in dict_path.values():\n",
    "        max_len = max(max_len, length)\n",
    "\n",
    "print('The maximum len shortest path is', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the maximum len shortest path is 2, Sheldon is **right**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10**. (5 points) Who is the character through whom the largest number of these indirect familiarities happen? Calculate an appropriate centrality metric on the familiarity graph to answer this question. You can use the package networkx for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sheldon': 0.16588466131033303,\n",
       " 'Leonard': 0.1651959033901968,\n",
       " 'Penny': 0.12612318851557294,\n",
       " 'Howard': 0.13325482509787182,\n",
       " 'Raj': 0.12452916790528193,\n",
       " 'Leslie': 0.011119941735193989,\n",
       " 'Kripke': 0.011218565139985724,\n",
       " 'Beverley': 0.01151397559372905,\n",
       " 'Stuart': 0.03050772347809307,\n",
       " 'Bernadette': 0.06934075746493526,\n",
       " 'Wil': 0.012630415131603711,\n",
       " 'Zack': 0.012126095717751528,\n",
       " 'Amy': 0.07903837805323158,\n",
       " 'Priya': 0.013382031011673692,\n",
       " 'Arthur': 0.009944061638076637,\n",
       " 'Bert': 0.011766323073894556,\n",
       " 'Emily': 0.012423985742574663}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.pagerank(familirity_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **pagerank centrality**, representing the importance of a node, this appears to be **Sheldon**, a main character in the show.\n",
    "\n",
    "Note: I wanted to use *betweeness centrality* which would make more sense IMO, but the top nodes have the same value. An assumption is that the wanted concept of episodes may be different than the one I assumed, i.e. The same location in the same episode is the same scene and this could lead to a different familiarity graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11**. (2.5 points) Another claim of Sheldon's is that every recurrent character in the show gossips about all the other recurrent characters. What property of the gossip graph would correspond to this? Does the gossip graph possess that property? If not, then is it the case that for every pair of recurrent characters, at least one gossips about the other? What property would this correspond to?\n",
    "\n",
    "The Gossip graph would be **Complete**. But one can see below that there's some character that don't gossip about one another (0 outside of the diagonale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 485, 303, 130,  87,   7,  27,   3,  33,  39,  54,   7, 231,\n",
       "          9,  14,  12,   5],\n",
       "       [441,   0, 266, 156, 113,  16,   9,   2,  35,  28,  31,  11,  60,\n",
       "         19,   5,   5,   4],\n",
       "       [320, 291,   0,  72,  64,   2,   1,   1,  14,  49,  14,  15, 103,\n",
       "         17,   2,   5,   8],\n",
       "       [164,  94,  61,   0, 114,   5,   2,   0,  26,  83,  14,   1,  20,\n",
       "          4,   2,   2,  12],\n",
       "       [126,  74,  63,  99,   0,   2,   4,   0,  25,  42,   9,   2,  27,\n",
       "          7,   0,   5,  37],\n",
       "       [  7,   7,   3,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [  1,   0,   0,   1,   1,   0,   0,   0,   0,   0,   1,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [  8,   7,   2,   1,   1,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,   0,   0,   0],\n",
       "       [ 19,  11,   8,  15,   7,   0,   0,   0,   0,   9,   5,   0,   2,\n",
       "          0,   0,   0,   0],\n",
       "       [ 48,  31,  38,  82,  49,   0,   0,   0,  26,   0,   1,   0,  34,\n",
       "          6,   0,   1,   8],\n",
       "       [ 10,   4,   4,   1,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,   0,   0,   0],\n",
       "       [  0,   2,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [266,  57,  73,  18,  25,   0,   3,   0,  10,  44,   8,   6,   0,\n",
       "          4,   2,   9,   0],\n",
       "       [  9,  13,   3,   5,   6,   0,   0,   0,   0,   2,   0,   0,   0,\n",
       "          0,   0,   0,   0],\n",
       "       [  8,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,   0,   0,   0],\n",
       "       [  4,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0,   0,   4,\n",
       "          0,   3,   0,   0],\n",
       "       [  1,   1,   1,   1,   8,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix_gossip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph would be semi-connected and the length of one of the path for each pair should be 1, which isn't the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doesn't hold for  4 14\n",
      "Property 2 holds : False\n"
     ]
    }
   ],
   "source": [
    "gossip_path = dict(nx.shortest_path_length(gossip_graph))\n",
    "\n",
    "def property_2(path):\n",
    "    for i in range(nb_char):\n",
    "        for j in range(nb_char):\n",
    "            if i != j and path[i][j] > 1 and path[j][i] > 1:\n",
    "                print(\"Doesn't hold for \", i, j)\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "print('Property 2 holds :', property_2(gossip_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12**. (5 points) Use the gossip graph and the familiarity graph to figure out if for every pair of recurrent characters, one of them has gossiped about the other if and only if they know each other. Explain your method - the simpler, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Raj', 'Arthur') isnt in gossip edges\n",
      "('Bernadette', 'Bert') isnt in familirity edges\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gossip_graph = nx.relabel_nodes(gossip_graph, dict(enumerate(rec_char_c)))\n",
    "\n",
    "def know_imply_gossip(familirity, gossip):\n",
    "    for a, b in familirity.edges:\n",
    "        if (a, b) not in gossip.edges and (b, a) not in gossip.edges:\n",
    "            print((a, b), 'isnt in gossip edges')\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def gossip_imply_know(familirity, gossip):\n",
    "    for e in gossip.edges:\n",
    "        if e not in familirity.edges:\n",
    "            print(e, 'isnt in familirity edges')\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "know_imply_gossip(familirity_graph, gossip_graph), gossip_imply_know(familirity_graph, gossip_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply check for the existing pair of edges if the properties 'one of them gossip => know each other' and 'know each other => one of them gossip' holds. As it's not the case for both graph, we concludes that the property **doesn't hold**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task D: The Detective's Hat (30 points)\n",
    "\n",
    "Sheldon claims that given a dialogue line, he can, with an accuracy of above 70%, say whether it's by himself or by someone else. Leonard contests this claim, since he believes that this claimed accuracy is too high. Leonard also suspects that it's easier for Sheldon to distinguish the lines that _aren't_ his, rather than those that _are_. We want you to put on the (proverbial) detective's hat and to investigate this claim.\n",
    "\n",
    "**Q13**. (7.5 points) Divide the set of all dialogue lines into two subsets: the training set, consisting of all the seasons except the last two, and the test set, consisting of the last two seasons. Each of your data points (which is one row of your matrix) is one **dialogue line**. Now, use the scikit-learn class **TfIdfVectorizer** to create TF-IDF representations for the data points in your training and test sets. Note that since you're going to train a machine learning model, everything used in the training needs to be independent of the test set. As a preprocessing step, remove stopwords and words that appear only once from your vocabulary. Use the simple tokenizer provided in `helpers/helper_functions.py` as an input to the TfidfVectorizer class, and use the words provided in `helpers/stopwords.txt` as your stopwords.\n",
    "\n",
    "Note: I use the DataFrame resulting from B, that is with only reccurent char + lowercase + punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_rec[df_rec.season < 9].copy()\n",
    "test_df = df_rec[df_rec.season >= 9].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('helpers/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    STOPWORDS = [w.strip() for w in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=simple_tokeniser, stop_words=STOPWORDS, min_df=2)\n",
    "train_tfidf = vectorizer.fit_transform(train_df.line_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14**. (5 points) Find the set of all words in the training set that are only uttered by Sheldon. Is it possible for Sheldon to identify himself only based on these? Use the test set to assess this possibility, and explain your method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5133, 12957, 13869)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_sheldon = set(train_df[train_df.character == 'Sheldon'].line_processed.str.split('\\W+').explode())\n",
    "words_other = set(train_df[train_df.character != 'Sheldon'].line_processed.str.split('\\W+').explode())\n",
    "words_only_sheldon = words_sheldon - words_other\n",
    "len(words_only_sheldon), len(words_sheldon), len(words_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that more than 5k words are only spoken by Sheldon, so that means that he has quite a unique vocabulary. Thus we try to check on the test set. If a sentence contains a word uttered only by Sheldon, then it's from him, otherwise, it's not, let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using our method we reach 78.002% accuracy\n"
     ]
    }
   ],
   "source": [
    "def from_Sheldon(x):\n",
    "    for token in x.split('\\W+'):\n",
    "        if token in words_only_sheldon:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "pred = test_df.line_processed.apply(from_Sheldon)\n",
    "accuracy = (pred == (test_df.character == 'Sheldon')).mean()*100\n",
    "print(f'Using our method we reach {accuracy:.3f}% accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this simple method we still reach 78% accuracy on the test set which is already better than what expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15**. (17.5 points) Now, perform singular value decomposition (SVD) on the training TF-IDF matrix, and calculate a **25-dimensional approximation** for both the training and test TF-IDF matrices (you can do this using scikit-learn's **TruncatedSVD** class). Then, train a logistic regression classifier with 10-fold cross-validation (using the scikit-learn **LogisticRegressionCV** class) on the output of the SVD that given a dialogue line, tells you whether it's by Sheldon or by someone else.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**i)** (7.5 points) Report precision, recall and F1-score for both classes (Sheldon and not-Sheldon), as well as accuracy, of your classifier on the training set and the test set. You need to implement the calculation of the evaluation measures (precision, etc.) yourself -- using the scikit-learn functions for them is not allowed.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**ii)** (5 points) What difference do you observe between the model's scores on the training and test sets? What could you infer from the amount of difference you see? What about the difference between scores on the two classes? Given the performance of your classifier, is Leonard right that the accuracy Sheldon claims is unattainable? What about his suspicions about the lines that Sheldon can and cannot distinguish?\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**iii)** (2.5 points) List 10 of the most extreme false positives and 10 of the most extreme false negatives, in terms of the probabilities predicted by the logistic regression model. What are common features of false positives? What about the false negatives?\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**iv)** (2.5 points) What is the most important feature in the model? What are the 5 most important words in this feature? _Hint: Think of the definition of an SVD, and that you did an SVD on the TF-IDF matrix with dialogue lines as rows and words as columns. You have projected the original data points onto a 25-dimensional subspace -- you need to look at the unit vectors you used for the projection._\n",
    "\n",
    "**i)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TF-IDF for the test set\n",
    "test_tfidf = vectorizer.transform(test_df.line_processed)\n",
    "\n",
    "# Create the Truncated SVD for the train and test set\n",
    "svd = TruncatedSVD(n_components=25)\n",
    "train_tfidf_trunc = svd.fit_transform(train_tfidf)\n",
    "test_tfidf_trunc = svd.transform(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "clf = LogisticRegressionCV(cv=10, random_state=1).fit(train_tfidf_trunc, train_df.character == 'Sheldon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    ''' Compute tp + tn / nb_elem '''\n",
    "    return (pred == target).mean()\n",
    "\n",
    "def precision(pred, target, label):\n",
    "    ''' Compute either tp / (tp + fp) (label=True) or tn / (tn + fn) (label=False)'''\n",
    "    true_pred_label = ((pred == label) & (target == label)).sum()\n",
    "    pred_label = (pred == label).sum()\n",
    "    return true_pred_label / pred_label\n",
    "\n",
    "def recall(pred, target, label):\n",
    "    ''' Compute either tp / (tp + fn) (label=True) or tn / (tn + fp) (label=False)'''\n",
    "    true_pred_label = ((pred == label) & (target == label)).sum()\n",
    "    true_label = (target == label).sum()\n",
    "    return true_pred_label / true_label\n",
    "\n",
    "def F1_score(precision, recall):\n",
    "    return (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = clf.predict(train_tfidf_trunc)\n",
    "target_train = train_df.character == 'Sheldon'\n",
    "pred_test = clf.predict(test_tfidf_trunc)\n",
    "target_test = test_df.character == 'Sheldon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Sheldon: True\n",
      "Train precision: 0.5384615384615384\n",
      "Train recall: 0.019255184088023698\n",
      "Train F1: 0.037180796731358524\n",
      "\n",
      "test precision: 0.4246575342465753\n",
      "test recall: 0.013857845328565042\n",
      "test F1: 0.02683982683982684\n",
      "\n",
      "Class Sheldon: False\n",
      "Train precision: 0.7550146674066439\n",
      "Train recall: 0.9945691906005222\n",
      "Train F1: 0.8583919235622859\n",
      "\n",
      "test precision: 0.7814976228209192\n",
      "test recall: 0.9947049924357034\n",
      "test F1: 0.8753050809851342\n",
      "\n",
      "Overall train accuracy: 0.7530974146737565\n",
      "Overall test accuracy: 0.7789359819057922\n"
     ]
    }
   ],
   "source": [
    "def print_measures(label):\n",
    "    print('Class Sheldon:', label)\n",
    "    train_precision = precision(pred_train, target_train, label)\n",
    "    train_recall = recall(pred_train, target_train, label)\n",
    "    train_f1 = F1_score(train_precision, train_recall)\n",
    "    print('Train precision:', train_precision)\n",
    "    print('Train recall:', train_recall)\n",
    "    print('Train F1:', train_f1)\n",
    "    print()\n",
    "    test_precision = precision(pred_test, target_test, label)\n",
    "    test_recall = recall(pred_test, target_test, label)\n",
    "    test_f1 = F1_score(test_precision, test_recall)\n",
    "    print('test precision:', test_precision)\n",
    "    print('test recall:', test_recall)\n",
    "    print('test F1:', test_f1)\n",
    "    print()\n",
    "\n",
    "print_measures(True)\n",
    "print_measures(False)\n",
    "print('Overall train accuracy:', accuracy(pred_train, target_train))\n",
    "print('Overall test accuracy:', accuracy(pred_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii)** Analysis is left to the reader but should be pretty clear that as the class are imbalanced it's easier to predict the dominating class aka not Sheldon and we see that in the difference in precision / recall for the 2 classes\n",
    "\n",
    "**iii)** Assumed to be on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proba = clf.predict_proba(test_tfidf_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_idx = test_proba[:, 0].argsort()[pred_test & ~target_test][::-1][:10]\n",
    "fn_idx = test_proba[:, 1].argsort()[~pred_test & target_test][::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 worst False positive\n",
      "Text: Sheldon says he’s sorry. Proba: [0.96492864 0.03507136]\n",
      "Text: Really? Proba: [0.91983019 0.08016981]\n",
      "Text: Sure. Proba: [0.90030485 0.09969515]\n",
      "Text: Oh, great, it’ll be easier for you to look up at the stars without a roof over your head. Proba: [0.84865358 0.15134642]\n",
      "Text: Really? So you can control it? Proba: [0.84160588 0.15839412]\n",
      "Text: Oh, you get used to it. Some guys you have to turn down over and over. Proba: [0.83259989 0.16740011]\n",
      "Text: Oh, I don’t know, I mean, let’s say we succeed in proving that our consciousness creates reality. I mean, what will we have really accomplished? You know, a loaf of bread’s still three bucks. Proba: [0.81064419 0.18935581]\n",
      "Text: I got the blues. My baby done left me. Proba: [0.80443036 0.19556964]\n",
      "Text: Oh, of course. Wouldn’t want to intrude. This is yours. Proba: [0.78203092 0.21796908]\n",
      "Text: I can’t believe you’re seeing someone and we don’t even know about it. Proba: [0.78128226 0.21871774]\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 worst False positive')\n",
    "for idx in fp_idx:\n",
    "    print('Text:', test_df.iloc[idx]['line'], 'Proba:', test_proba[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 worst False negative\n",
      "Text: Hello, Leonard. Proba: [0.28446547 0.71553453]\n",
      "Text: Leonard. Proba: [0.28886415 0.71113585]\n",
      "Text: Uh, I’m Leonard. This is my friend… Proba: [0.28886415 0.71113585]\n",
      "Text: I’m with Leonard. Proba: [0.28886415 0.71113585]\n",
      "Text: Leonard? Proba: [0.28886415 0.71113585]\n",
      "Text: Leonard? Leonard? Proba: [0.28886415 0.71113585]\n",
      "Text: Leonard. Proba: [0.28886415 0.71113585]\n",
      "Text: Leonard? Proba: [0.28886415 0.71113585]\n",
      "Text: (Knock, knock, knock) Leonard. (Knock, knock, knock) Leonard. (Knock, knock, knock) Leonard. Proba: [0.41120164 0.58879836]\n",
      "Text: Penny? Proba: [0.41791846 0.58208154]\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 worst False negative')\n",
    "for idx in fn_idx:\n",
    "    print('Text:', test_df.iloc[idx]['line'], 'Proba:', test_proba[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis is left to the reader\n",
    "\n",
    "**iv)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.55704617, -2.71105222, -2.06113349, -1.04050008,  0.92629362,\n",
       "        -0.82198275,  0.08895675, -0.83925964,  0.02577902,  0.75272917,\n",
       "        -1.15427775,  0.06574843, -0.8977394 , -0.70522051, -0.42137013,\n",
       "         2.24716611, -0.9367863 ,  0.05341557, -0.36654813, -1.54983582,\n",
       "        -2.67095425, -0.76064782, -0.77470872,  1.00821925, -0.26772402]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: it makes sense that first feature is the most important as it's the first vector associated to the first singular value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words in Feature 0\n",
      "Word rank: 1 Word: sheldon\n",
      "Word rank: 2 Word: hey\n",
      "Word rank: 3 Word: know\n",
      "Word rank: 4 Word: well\n",
      "Word rank: 5 Word: go\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 words in Feature 0')\n",
    "for i, idx in enumerate(svd.components_[0].argsort()[::-1][:5]):\n",
    "    for term, term_idx in vectorizer.vocabulary_.items():\n",
    "        if term_idx == idx:\n",
    "            print('Word rank:', i+1, 'Word:', term)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
